{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHANOIR api access\n",
    "\n",
    "API documentation available at \n",
    "\n",
    "https://github.com/fli-iam/shanoir-ng/wiki \n",
    "\n",
    "https://github.com/fli-iam/shanoir-ng/wiki/REST-Tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJNQXBrRkRMZ2VrS1cxTGI0em1LcDh1VGlDbElWNXZiOTdhckE3c25DbmFJIn0.eyJleHAiOjE2MjE1OTA2MDAsImlhdCI6MTYyMTU5MDMwMCwianRpIjoiNGYzNGZlNTMtNzVjZS00NWM2LWE1NGUtYWE3Y2Y2ODZlODZlIiwiaXNzIjoiaHR0cHM6Ly9zaGFub2lyLmlyaXNhLmZyL2F1dGgvcmVhbG1zL3NoYW5vaXItbmciLCJhdWQiOiJhY2NvdW50Iiwic3ViIjoiYzUwMWI1MmUtZDBlYy00ZTlkLWJhNGQtYWM4Mjc4YjMyODFmIiwidHlwIjoiQmVhcmVyIiwiYXpwIjoic2hhbm9pci11cGxvYWRlciIsInNlc3Npb25fc3RhdGUiOiIzMDQxMzg4YS04MzM1LTQ2MzgtYmIwMi05YTI0MWJlOTAyNTciLCJhY3IiOiIxIiwicmVhbG1fYWNjZXNzIjp7InJvbGVzIjpbIm9mZmxpbmVfYWNjZXNzIiwiUk9MRV9FWFBFUlQiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmFtZSI6IkFsYmFuIEdhaWduYXJkIiwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWdhaWduYXJkIiwiZ2l2ZW5fbmFtZSI6IkFsYmFuIiwiZmFtaWx5X25hbWUiOiJHYWlnbmFyZCIsInVzZXJJZCI6MjIwLCJlbWFpbCI6ImFsYmFuLmdhaWduYXJkQHVuaXYtbmFudGVzLmZyIn0.Z1toFUwoto0QenJl-6XkDCWWLVXo4Hy9aYiEkIxheTVWlsMV51MNzJUAXR5EwI9XUjyjdbhMcSUb6f2yT86JKl9-JWx7NuZDxdRHsdl7s03Lso5C2FNZn2Wa5-HCDE-1A8UaizwliHIqXlEWntD3xZ61pwvWYbaZpVylvlI5cJz_rgGKxh_BNp4v08WqhF1CFjrHgUn-uuIcPW3-3Kt8knHgMwpnOXXgTBT8giz0YmyHvYWEye4sSihfnb1Lokkm6jEABgZ1VIClf7CbfS495OPYDkXBxIpgCn_L3bpddM76HXBisXvghLbMvwgtmnWSOQTAUa4xIM1Ms27o9Bh6iA'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def get_token():\n",
    "    r = requests.post('https://shanoir.irisa.fr/auth/realms/shanoir-ng/protocol/openid-connect/token', \n",
    "                  data = {'client_id':'shanoir-uploader', \n",
    "                          'grant_type':'password', \n",
    "                          'username':'LOGIN', \n",
    "                          'password':'PASSWORD', \n",
    "                          'scope':'openid'})\n",
    "\n",
    "    answer = r.json()\n",
    "    token = answer['access_token']\n",
    "    return token\n",
    "\n",
    "get_token()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Subjects \n",
    "Here we build two index to bridge inclusion identifiers with internal shanoir identifiers. <br>\n",
    "The example below requests data for the study ICAN (study id : 124)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subjects():\n",
    "    api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/subjects/124/allSubjects'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(api_url, headers=h)\n",
    "    json_res = r.json()\n",
    "    subjects_shanoir_id = {}\n",
    "    subjects_ican_id = {}\n",
    "    for s in json_res:\n",
    "        subjects_ican_id[str(s['id'])] = str(s['name'])\n",
    "        subjects_shanoir_id[str(s['name'])] = str(s['id'])\n",
    "    return subjects_shanoir_id, subjects_ican_id\n",
    "\n",
    "subjects_shanoir_id, subjects_ican_id = get_all_subjects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing and downloading Shanoir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_subjects(study_id):\n",
    "    api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/subjects/{study_id}/allSubjects'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(api_url, headers=h)\n",
    "    json_res = r.json()\n",
    "    subjects_shanoir_id = {}\n",
    "    subjects_study_id = {}\n",
    "    for s in json_res:\n",
    "        subjects_study_id[str(s['id'])] = str(s['name'])\n",
    "        subjects_shanoir_id[str(s['name'])] = str(s['id'])\n",
    "    return subjects_shanoir_id, subjects_study_id\n",
    "\n",
    "def get_all_subjectsId(study_id):\n",
    "    api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/subjects/{study_id}/allSubjects'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(api_url, headers=h)\n",
    "    json_res = r.json()\n",
    "    res = []\n",
    "    for subject in json_res:\n",
    "      res.append(subject['id'])\n",
    "    return res\n",
    "\n",
    "def get_acquisitions(subject_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/datasets/examinations/subject/{subject_id}/study/124'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "    #print(r.text)\n",
    "    json_res = r.json()\n",
    "    res = []\n",
    "    for exam in json_res:\n",
    "        #print(exam)\n",
    "        for acquisition in exam['datasetAcquisitions']:\n",
    "            res.append(acquisition['id'])\n",
    "    return res\n",
    "\n",
    "def get_acquisition(acquisition_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/datasets/datasetacquisition/{acquisition_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "    #print(r.text)\n",
    "\n",
    "    acq = r.json()\n",
    "    print(json.dumps(acq, indent=True))\n",
    "    print(acq['protocol'])\n",
    "\n",
    "def get_examinations(subject_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/datasets/examinations/subjectid/{subject_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    exams = r.json()\n",
    "    return(exams)\n",
    "    #print(json.dumps(exams, indent=True))\n",
    "\n",
    "def get_examinationsByStudy(study_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/datasets/examinations/study/{study_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    exams = r.json()\n",
    "    return(exams)\n",
    "    #print(json.dumps(exams, indent=True))\n",
    "\n",
    "def get_datasets(subject_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/datasets/datasets/subject/{subject_id}/study/124'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "    datasets = r.json()\n",
    "    res = []\n",
    "    for d in datasets:\n",
    "        res.append(d)\n",
    "    return(res)\n",
    "\n",
    "\n",
    "def get_centerNames():\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/studies/namesAndCenters'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "    studies = r.json()\n",
    "    # Suppression des redondances entre les centres ican et ucan\n",
    "    unique_center_names = set()\n",
    "\n",
    "    for study in studies:\n",
    "        for study_center in study.get(\"studyCenterList\", []):\n",
    "            center_name = study_center.get(\"center\", {}).get(\"name\")\n",
    "            if center_name:\n",
    "                unique_center_names.add(center_name)\n",
    "\n",
    "    return list(unique_center_names)\n",
    "\n",
    "\n",
    "def get_center(center_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/centers/{center_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    center = r.json()\n",
    "    return(center)\n",
    "\n",
    "\n",
    "def get_dataset(dataset_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/datasets/datasets/{dataset_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "    #print(r.text)\n",
    "\n",
    "    dataset = r.json()\n",
    "    return(dataset)\n",
    "#    print(json.dumps(dataset, indent=True))\n",
    "\n",
    "def get_studyStatistics(study_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/studies/statistics/{study_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    dataset = r.json()\n",
    "    return(dataset)\n",
    "\n",
    "\n",
    "def get_dicomMetadata(dataset_id):\n",
    "    dataset_api_url = f'https://shanoir-qualif.irisa.fr/shanoir-ng/datasets/datasets/dicom-metadata/{dataset_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "    if r.status_code == 200:\n",
    "      dicom_metadata = r.json()\n",
    "      return(dicom_metadata)\n",
    "    elif r.status_code == 500:\n",
    "        print(\"Internal Server Error (500)\")\n",
    "        return(None)\n",
    "\n",
    "\n",
    "def download_dataset(subject_name, dataset_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/datasets/datasets/download/{dataset_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    #print(r.status_code)\n",
    "    #print(r.headers['content-type'])\n",
    "    #print(r.encoding)\n",
    "\n",
    "    if (r.status_code == 200):\n",
    "        path = f'./{subject_name}/{dataset_id}'\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(path+'/file.zip', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        with zipfile.ZipFile(path+'/file.zip','r') as zip_ref:\n",
    "            zip_ref.extractall(path)\n",
    "        os.remove(path+'/file.zip')\n",
    "        print(f'successfully downloaded dataset to {path}')\n",
    "    else :\n",
    "        print(r.text)\n",
    "\n",
    "def download_datasets(filepath, datasets_ids):\n",
    "    dataset_api_url = f'https://shanoir-qualif.irisa.fr/shanoir-ng/datasets/datasets/massiveDownload/{datasets_ids}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer '+get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    if (r.status_code == 200):\n",
    "        path = f'{filepath}'\n",
    "        #os.makedirs(path, exist_ok=True)\n",
    "        with open(path+'/file.zip', 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        with zipfile.ZipFile(path+'/file.zip','r') as zip_ref:\n",
    "            zip_ref.extractall(path)\n",
    "        os.remove(path+'/file.zip')\n",
    "        print(f'successfully downloaded datasets to {path}')\n",
    "    else :\n",
    "        print(r.text)\n",
    "\n",
    "def get_studyCenters(study_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/centers/names/{study_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer ' + get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    return r.json()\n",
    "\n",
    "def get_subjectsNames():\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/subjects/names'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer ' + get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    return r.json()\n",
    "\n",
    "def get_subject(subject_id):\n",
    "    dataset_api_url = f'https://shanoir.irisa.fr/shanoir-ng/studies/subjects/{subject_id}'\n",
    "    h = {\"Accept\": \"application/json\", 'Authorization': 'Bearer ' + get_token()}\n",
    "    r = requests.get(dataset_api_url, headers=h)\n",
    "\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ids = ['AIC_25_0118',\n",
    "'AIC_01_0502']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74178e1db47341b9be870166c8e829f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "AIC_25_0118\n",
      "9677\n",
      "379659 : T2 TSE AX\n",
      "379660 : TOF 3D WILLIS\n",
      "TOF image available for download\n",
      "--------\n",
      "AIC_01_0502\n",
      "9425\n",
      "375223 : 375223 2012-10-29 MR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for inclusion_id in tqdm(list_of_ids):\n",
    "    print('--------')\n",
    "    print(inclusion_id)\n",
    "    if inclusion_id in subjects_shanoir_id.keys():\n",
    "        print(subjects_shanoir_id[inclusion_id])\n",
    "        for dataset_id in get_datasets(subjects_shanoir_id[inclusion_id]):\n",
    "            dataset = get_dataset(dataset_id)\n",
    "            #print(dataset)\n",
    "            print(f'{dataset_id} : {dataset[\"name\"]}')    \n",
    "            if ('tof' in dataset['name'].lower()) or ('angio' in dataset['name'].lower()):\n",
    "                  print(f'TOF image available for download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RETRIEVING INFORMATIONS FROM A SHANOIR SUBJECT IDS LIST ###\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import FileLink\n",
    "\n",
    "## OBJECTIVE : RESEARCHER WITH A LIST OF SUBJECT IDS WANTS TO RETRIEVE THE FOLLOWING FIELDS ASSOCIATED WITH SHANOIR IDS: \n",
    "# Name of the study\n",
    "# Name of the center where TOF exam occured\n",
    "# Date of TOF exam\n",
    "\n",
    "# Creation of a subject's names list from original file (with 1 subject name per line without header)\n",
    "with open('{FILEPATH}', 'r') as file:\n",
    "    namesList = [line.strip() for line in file]\n",
    "\n",
    "print(namesList)\n",
    "\n",
    "# Getting Shanoir's subjects list\n",
    "subjects_df = pd.DataFrame(get_subjectsNames())\n",
    "name_to_id = {row['name']: row['id'] for index, row in subjects_df.iterrows()}\n",
    "\n",
    "# Creation of results dataframe\n",
    "results = pd.DataFrame(columns=['subject_name', 'study_name', 'center_name', 'exam_date'])\n",
    "\n",
    "# For each subject name of the original list, if this name matches a Shanoir subject id , we retrieve informations linked to this subject\n",
    "for subject_name in namesList :\n",
    "  if subject_name in name_to_id :\n",
    "    # Getting subject id linked to subject name\n",
    "    subject_id = name_to_id[subject_name]\n",
    "    # Getting informations from subject\n",
    "    subject_info = get_subject(subject_id)\n",
    "    # Getting study linked to the subject\n",
    "    study_name = subject_info[\"subjectStudyList\"][0][\"study\"][\"name\"]\n",
    "    # Getting exams from the subject\n",
    "    exams_data = get_examinations(subject_id)\n",
    "    exams_info = pd.DataFrame(exams_data)\n",
    "\n",
    "    # Converting column 'examinationDate' in datetime if not already\n",
    "    exams_info['examinationDate'] = pd.to_datetime(exams_info['examinationDate'])\n",
    "    # Find oldest exam date\n",
    "    index_oldest_exam = exams_info['examinationDate'].idxmin()\n",
    "    # Getting oldest exam\n",
    "    oldest_exam = exams_info.loc[index_oldest_exam]\n",
    "\n",
    "    center_id = None\n",
    "    exam_date = None\n",
    "\n",
    "    # Getting infos from oldest TOF exam\n",
    "    for row_index, row in exams_info.iterrows() :\n",
    "      if ('tof' in row['comment'].lower()) or ('angio' in row['comment'].lower()) or ('time of flight' in row['comment'].lower()):\n",
    "        center_id = row['centerId']\n",
    "        exam_date = row['examinationDate']\n",
    "        break\n",
    "    if center_id is None:\n",
    "      center_id = oldest_exam['centerId']\n",
    "      exam_date = oldest_exam['examinationDate']\n",
    "    \n",
    "    # Getting center name from center id\n",
    "    center_info = get_center(center_id)\n",
    "    center_name = center_info['name']\n",
    "\n",
    "    #results = results.append({'subject_name': subject_name, 'study_name': study_name, 'center_name': center_name, 'exam_date': exam_date}, ignore_index=True)\n",
    "    new_row = {'subject_name': subject_name, 'study_name': study_name, 'center_name': center_name, 'exam_date': exam_date}\n",
    "\n",
    "    results = pd.concat([results, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "results_str = results.to_string(index=False)\n",
    "print(results_str)\n",
    "\n",
    "# Saving as CSV\n",
    "#results.to_csv('results.csv', index=False)\n",
    "\n",
    "# Creating download link to csv file\n",
    "#FileLink('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for datasets export\n",
    "\n",
    "## OBJECTIVE\n",
    "Getting a list of dataset ids from different studies based on specific DICOM metadata values\n",
    "\n",
    "## RULES\n",
    "Images from different centers <br>\n",
    "+ At least 50 images in the batch of the sequence <br>\n",
    "+ AND slice thickness < 0.5 cm <br>\n",
    "+ AND terms: time of flight OR TOF or MRA or angiography <br>\n",
    "\n",
    "## METADATA\n",
    "ProtocolName 0018,1030 <br>\n",
    "SeriesDescription 0008,103E <br>\n",
    "SliceThickness 0018,0050 <br>\n",
    "NumberOfSlices 2001,1018 <br>\n",
    "NumberOfFrames 0028,0008 <br>\n",
    "DataElements 07A1,1002 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define number of datasets desired\n",
    "dataset_size = 10\n",
    "# define studies of interest 124 : ICAN, 178 : UCAN\n",
    "studies = {'124','178'}\n",
    "\n",
    "# filepath where datasets are downloaded\n",
    "filepath = ''\n",
    "\n",
    "all_subjectsId = []\n",
    "all_datasetsId = []\n",
    "results_datasetsId = []\n",
    "mri_types = [\"tof\",\"angio\",\"angiography\",\"time of flight\",\"mra\"]\n",
    "\n",
    "\n",
    "# GET ALL SUBJECTS OF STUDIES OF INTEREST\n",
    "for study in studies :\n",
    "  count = 0\n",
    "  # get a list of all subjects id for this study\n",
    "  all_subjectsId = get_all_subjectsId(study)\n",
    "  for subject in all_subjectsId :\n",
    "    # get a list of all datasets id from this subject id and this study id\n",
    "    all_datasetsId = get_datasets(subject, study)\n",
    "    for dataset in all_datasetsId :\n",
    "      is_tof = False\n",
    "      enough_frames = False\n",
    "      thin_enough = False\n",
    "      # get dicom metadata for each dataset and check quality compliance\n",
    "      metadata = get_dicomMetadata(dataset)\n",
    "      if metadata is not None :\n",
    "        for item in metadata:\n",
    "          # Check if ProtocolName or SeriesDescription contains \"tof\" or \"angio\" or \"flight\"\n",
    "          if ('0008103E' in item and any(x in item['0008103E'][\"Value\"][0].lower() for x in mri_types)) or ('00181030' in item and any(x in item['00181030'][\"Value\"][0].lower() for x in mri_types)):\n",
    "            is_tof = True\n",
    "\n",
    "          if \"00180050\" in item and item[\"00180050\"][\"Value\"] != [] and float(item[\"00180050\"][\"Value\"][0]) < 5:\n",
    "            thin_enough = True\n",
    "\n",
    "          if (\"20011018\" in item and item[\"20011018\"][\"Value\"] != [] and int(item[\"20011018\"][\"Value\"][0]) > 50) or (\"00280008\" in item and item[\"00280008\"][\"Value\"] != [] and int(item[\"00280008\"][\"Value\"][0]) > 50) or (\"07A11002\" in item and item[\"07A11002\"][\"Value\"] != [] and int(item[\"07A11002\"][\"Value\"][0]) > 50):\n",
    "            enough_frames = True\n",
    "\n",
    "        if is_tof and thin_enough and enough_frames :\n",
    "          # Avoid duplicates in case subjects in multiple studies\n",
    "          if dataset not in results_datasetsId :\n",
    "            results_datasetsId.append(dataset)\n",
    "            count += 1\n",
    "          if count == dataset_size/(len(studies)) :\n",
    "            break\n",
    "    if count == dataset_size/(len(studies)) :\n",
    "      break\n",
    "\n",
    "print(results_datasetsId)\n",
    "\n",
    "# download_datasets(filepath, results_datasetsId)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
